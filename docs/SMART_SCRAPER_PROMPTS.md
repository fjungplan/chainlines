# Smart Scraper Implementation Prompts

**Version:** 1.0  
**Date:** 2026-01-04  
**Parent Documents:**
- [SMART_SCRAPER_SPECIFICATION.md](./SMART_SCRAPER_SPECIFICATION.md)
- [SMART_SCRAPER_IMPLEMENTATION_BREAKDOWN.md](./SMART_SCRAPER_IMPLEMENTATION_BREAKDOWN.md)

---

## How to Use This Document

Each slice below is a **self-contained "fire-and-forget" prompt** for a code-generation LLM. Copy the entire slice (including context) and paste it as your prompt.

**Key Principles:**
1. **Test-First (TDD):** Always write the test before the implementation.
2. **Incremental:** Each slice builds on the previous.
3. **No Orphan Code:** Every new file is immediately wired into existing code.
4. **Auto-Commit:** Each slice ends with automatic git commit execution.
5. **Task Tracking:** Each slice updates `docs/SMART_SCRAPER_TASKS.md` with progress.

**Workflow per Slice:**
1. Copy prompt â†’ Paste to LLM
2. LLM implements all tasks with TDD
3. LLM runs tests
4. LLM updates task checklist
5. LLM commits changes automatically

**Related Files:**
- [SMART_SCRAPER_TASKS.md](./SMART_SCRAPER_TASKS.md) - Progress checklist (updated by each slice)
- [SMART_SCRAPER_SPECIFICATION.md](./SMART_SCRAPER_SPECIFICATION.md) - Technical specification
- [SMART_SCRAPER_IMPLEMENTATION_BREAKDOWN.md](./SMART_SCRAPER_IMPLEMENTATION_BREAKDOWN.md) - Detailed breakdown


# SLICE 1: Foundation & Infrastructure

## Context
We are implementing a "Smart Scraper" for the Chainlines cycling database. This first slice establishes the foundation that all other work depends on: dependencies, database migration, and system user.

**Current Branch:** `smart-scraper`

**Relevant Existing Files:**
- `backend/requirements.txt` - Python dependencies
- `backend/app/models/sponsor.py` - Contains `TeamSponsorLink` with prominence constraint
- `backend/alembic/versions/` - Migration files

## Prompt

You are implementing SLICE 1 of the Smart Scraper project. Follow TDD strictly.

### TASK 1.1: Add Dependencies (Test: Installation Works)

**Test First:**
Create `backend/tests/scraper/test_dependencies.py`:
```python
"""Test that scraper dependencies are installed correctly."""
import pytest

def test_instructor_installed():
    import instructor
    assert instructor is not None

def test_google_generativeai_installed():
    import google.generativeai
    assert google.generativeai is not None

def test_openai_installed():
    import openai
    assert openai is not None
```

**Implementation:**
Update `backend/requirements.txt` to add:
```
instructor>=1.0.0
google-generativeai>=0.3.0
openai>=1.0.0
```

**Verify:** Run `pip install -r backend/requirements.txt` then `pytest backend/tests/scraper/test_dependencies.py -v`

---

### TASK 1.2: Database Migration - Relax Prominence Constraint (Test: 0% Allowed)

**Test First:**
Create `backend/tests/scraper/test_prominence_constraint.py`:
```python
"""Test that 0% prominence is now allowed."""
import pytest
from app.models.sponsor import TeamSponsorLink

def test_zero_prominence_allowed():
    """Validate that 0% prominence passes validation."""
    # This should NOT raise ValueError
    link = TeamSponsorLink.__new__(TeamSponsorLink)
    result = link.validate_prominence("prominence_percent", 0)
    assert result == 0

def test_negative_prominence_rejected():
    """Validate that negative prominence is still rejected."""
    link = TeamSponsorLink.__new__(TeamSponsorLink)
    with pytest.raises(ValueError):
        link.validate_prominence("prominence_percent", -1)

def test_over_100_rejected():
    """Validate that >100% is still rejected."""
    link = TeamSponsorLink.__new__(TeamSponsorLink)
    with pytest.raises(ValueError):
        link.validate_prominence("prominence_percent", 101)
```

**Implementation:**

1. Update `backend/app/models/sponsor.py` - Change the validator:
```python
@validates("prominence_percent")
def validate_prominence(self, key, value):
    if value is not None:
        if value < 0 or value > 100:  # Changed from <= 0
            raise ValueError("prominence_percent must be between 0 and 100")
    return value
```

2. Create Alembic migration:
```bash
cd backend
alembic revision -m "relax_prominence_constraint_allow_zero"
```

3. Edit the new migration file:
```python
"""relax_prominence_constraint_allow_zero"""

from alembic import op

def upgrade() -> None:
    # Drop old constraint
    op.drop_constraint('check_prominence_range', 'team_sponsor_link', type_='check')
    # Add new constraint allowing 0
    op.create_check_constraint(
        'check_prominence_range',
        'team_sponsor_link',
        'prominence_percent >= 0 AND prominence_percent <= 100'
    )

def downgrade() -> None:
    op.drop_constraint('check_prominence_range', 'team_sponsor_link', type_='check')
    op.create_check_constraint(
        'check_prominence_range',
        'team_sponsor_link',
        'prominence_percent > 0 AND prominence_percent <= 100'
    )
```

**Verify:** 
- Run `alembic upgrade head`
- Run `pytest backend/tests/scraper/test_prominence_constraint.py -v`

---

### TASK 1.3: System Bot User Seed Script (Test: User Exists)

**Test First:**
Create `backend/tests/scraper/test_system_user.py`:
```python
"""Test that Smart Scraper system user exists after seeding."""
import pytest
from uuid import UUID
from sqlalchemy import select
from app.models.user import User

SMART_SCRAPER_USER_ID = UUID("00000000-0000-0000-0000-000000000001")

@pytest.mark.asyncio
async def test_smart_scraper_user_exists(isolated_session):
    """After seeding, the Smart Scraper user should exist."""
    from app.db.seed_smart_scraper_user import seed_smart_scraper_user
    
    await seed_smart_scraper_user(isolated_session)
    await isolated_session.commit()
    
    result = await isolated_session.execute(
        select(User).where(User.user_id == SMART_SCRAPER_USER_ID)
    )
    user = result.scalar_one_or_none()
    
    assert user is not None
    assert user.username == "smart_scraper"
    assert user.email == "system@chainlines.local"
```

**Implementation:**
Create `backend/app/db/seed_smart_scraper_user.py`:
```python
"""Seed script for Smart Scraper system user."""
from uuid import UUID
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.user import User
from app.models.enums import UserRole

# Fixed UUID for reproducibility across environments
SMART_SCRAPER_USER_ID = UUID("00000000-0000-0000-0000-000000000001")

async def seed_smart_scraper_user(session: AsyncSession) -> User:
    """Create or retrieve the Smart Scraper system user."""
    result = await session.execute(
        select(User).where(User.user_id == SMART_SCRAPER_USER_ID)
    )
    existing = result.scalar_one_or_none()
    
    if existing:
        return existing
    
    user = User(
        user_id=SMART_SCRAPER_USER_ID,
        username="smart_scraper",
        email="system@chainlines.local",
        password_hash="SYSTEM_USER_NO_LOGIN",
        role=UserRole.ADMIN,  # Needs admin to create edits
    )
    session.add(user)
    return user
```

**Verify:** Run `pytest backend/tests/scraper/test_system_user.py -v`

---

### WIRING: Export the seed function

Update `backend/app/db/__init__.py` to include:
```python
from app.db.seed_smart_scraper_user import seed_smart_scraper_user, SMART_SCRAPER_USER_ID
```

---

## Finalize Slice 1

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 1.1 Add dependencies to `requirements.txt`
- [x] 1.2 Create Alembic migration for prominence constraint
- [x] 1.3 Update `TeamSponsorLink` model validator
- [x] 1.4 Create "Smart Scraper" user seed script
- [x] **SLICE 1 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add foundation infrastructure

- Add instructor, google-generativeai, openai dependencies
- Relax prominence constraint to allow 0% for technical partners
- Add Smart Scraper system user seed script"
```

---

# SLICE 2: LLM Client Layer

## Context
With foundation in place, we now build the LLM client layer. This provides connectivity to Gemini (primary) and Deepseek (fallback) with automatic failover.

**Dependencies:** SLICE 1 must be complete.

**Relevant Existing Files:**
- `backend/.env` - Should contain `GEMINI_API_KEY` and/or `DEEPSEEK_API_KEY`
- `backend/app/core/config.py` - Application configuration

## Prompt

You are implementing SLICE 2 of the Smart Scraper project. Follow TDD strictly.

### TASK 2.1: LLM Client Protocol (Test: Interface Defined)

**Test First:**
Create `backend/tests/scraper/test_llm_client.py`:
```python
"""Test LLM client protocol and implementations."""
import pytest
from typing import Protocol, runtime_checkable

def test_base_llm_client_is_protocol():
    """BaseLLMClient should be a runtime-checkable Protocol."""
    from app.scraper.llm.base import BaseLLMClient
    assert hasattr(BaseLLMClient, '__protocol_attrs__') or isinstance(BaseLLMClient, type)
```

**Implementation:**
Create `backend/app/scraper/llm/__init__.py` (empty).
Create `backend/app/scraper/llm/base.py`:
```python
"""Base LLM client protocol."""
from typing import Protocol, TypeVar, Type
from pydantic import BaseModel

T = TypeVar('T', bound=BaseModel)

class BaseLLMClient(Protocol):
    """Protocol for LLM clients."""
    
    async def generate_structured(
        self,
        prompt: str,
        response_model: Type[T],
        temperature: float = 0.1
    ) -> T:
        """Generate a structured response matching the Pydantic model."""
        ...
```

**Verify:** Run `pytest backend/tests/scraper/test_llm_client.py::test_base_llm_client_is_protocol -v`

---

### TASK 2.2: Gemini Client (Test: Mocked Response)

**Test First:**
Add to `backend/tests/scraper/test_llm_client.py`:
```python
from pydantic import BaseModel
from unittest.mock import AsyncMock, patch, MagicMock

class SimpleResponse(BaseModel):
    answer: str

@pytest.mark.asyncio
async def test_gemini_client_returns_structured():
    """GeminiClient should return structured Pydantic response."""
    from app.scraper.llm.gemini import GeminiClient
    
    # Mock the instructor-patched client
    with patch('app.scraper.llm.gemini.genai') as mock_genai:
        mock_model = MagicMock()
        mock_genai.GenerativeModel.return_value = mock_model
        
        with patch('app.scraper.llm.gemini.instructor') as mock_instructor:
            mock_client = AsyncMock()
            mock_client.chat.completions.create = AsyncMock(
                return_value=SimpleResponse(answer="test")
            )
            mock_instructor.from_gemini.return_value = mock_client
            
            client = GeminiClient(api_key="test-key")
            result = await client.generate_structured(
                prompt="What is 2+2?",
                response_model=SimpleResponse
            )
            
            assert isinstance(result, SimpleResponse)
            assert result.answer == "test"
```

**Implementation:**
Create `backend/app/scraper/llm/gemini.py`:
```python
"""Gemini LLM client implementation."""
from typing import Type, TypeVar
import google.generativeai as genai
import instructor
from pydantic import BaseModel

T = TypeVar('T', bound=BaseModel)

class GeminiClient:
    """Client for Google Gemini API with structured output."""
    
    def __init__(self, api_key: str, model: str = "gemini-2.5-pro"):
        genai.configure(api_key=api_key)
        self._model = genai.GenerativeModel(model)
        self._client = instructor.from_gemini(
            client=self._model,
            mode=instructor.Mode.GEMINI_JSON
        )
    
    async def generate_structured(
        self,
        prompt: str,
        response_model: Type[T],
        temperature: float = 0.1
    ) -> T:
        """Generate structured response."""
        return await self._client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            response_model=response_model,
            temperature=temperature
        )
```

**Verify:** Run `pytest backend/tests/scraper/test_llm_client.py::test_gemini_client_returns_structured -v`

---

### TASK 2.3: Deepseek Client (Test: Mocked Response)

**Test First:**
Add to `backend/tests/scraper/test_llm_client.py`:
```python
@pytest.mark.asyncio
async def test_deepseek_client_returns_structured():
    """DeepseekClient should return structured Pydantic response."""
    from app.scraper.llm.deepseek import DeepseekClient
    
    with patch('app.scraper.llm.deepseek.instructor') as mock_instructor:
        mock_client = AsyncMock()
        mock_client.chat.completions.create = AsyncMock(
            return_value=SimpleResponse(answer="deepseek-test")
        )
        mock_instructor.from_openai.return_value = mock_client
        
        client = DeepseekClient(api_key="test-key")
        result = await client.generate_structured(
            prompt="What is 2+2?",
            response_model=SimpleResponse
        )
        
        assert isinstance(result, SimpleResponse)
        assert result.answer == "deepseek-test"
```

**Implementation:**
Create `backend/app/scraper/llm/deepseek.py`:
```python
"""Deepseek LLM client implementation (OpenAI-compatible API)."""
from typing import Type, TypeVar
from openai import AsyncOpenAI
import instructor
from pydantic import BaseModel

T = TypeVar('T', bound=BaseModel)

class DeepseekClient:
    """Client for Deepseek API (OpenAI-compatible) with structured output."""
    
    def __init__(self, api_key: str, model: str = "deepseek-reasoner"):
        self._openai = AsyncOpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        self._client = instructor.from_openai(self._openai)
        self._model = model
    
    async def generate_structured(
        self,
        prompt: str,
        response_model: Type[T],
        temperature: float = 0.1
    ) -> T:
        """Generate structured response."""
        return await self._client.chat.completions.create(
            model=self._model,
            messages=[{"role": "user", "content": prompt}],
            response_model=response_model,
            temperature=temperature
        )
```

**Verify:** Run `pytest backend/tests/scraper/test_llm_client.py::test_deepseek_client_returns_structured -v`

---

### TASK 2.4: LLM Service with Fallback (Test: Fallback Triggers)

**Test First:**
Add to `backend/tests/scraper/test_llm_client.py`:
```python
@pytest.mark.asyncio
async def test_llm_service_fallback_on_error():
    """LLMService should fallback to secondary client on primary failure."""
    from app.scraper.llm.service import LLMService
    
    primary = AsyncMock()
    primary.generate_structured = AsyncMock(side_effect=Exception("Primary failed"))
    
    secondary = AsyncMock()
    secondary.generate_structured = AsyncMock(
        return_value=SimpleResponse(answer="fallback-worked")
    )
    
    service = LLMService(primary=primary, secondary=secondary)
    result = await service.generate_structured(
        prompt="test",
        response_model=SimpleResponse
    )
    
    assert result.answer == "fallback-worked"
    primary.generate_structured.assert_called_once()
    secondary.generate_structured.assert_called_once()

@pytest.mark.asyncio
async def test_llm_service_uses_primary_first():
    """LLMService should use primary when it succeeds."""
    from app.scraper.llm.service import LLMService
    
    primary = AsyncMock()
    primary.generate_structured = AsyncMock(
        return_value=SimpleResponse(answer="primary-worked")
    )
    
    secondary = AsyncMock()
    
    service = LLMService(primary=primary, secondary=secondary)
    result = await service.generate_structured(
        prompt="test",
        response_model=SimpleResponse
    )
    
    assert result.answer == "primary-worked"
    secondary.generate_structured.assert_not_called()
```

**Implementation:**
Create `backend/app/scraper/llm/service.py`:
```python
"""LLM Service with fallback chain."""
import logging
from typing import Type, TypeVar, Optional
from pydantic import BaseModel
from app.scraper.llm.base import BaseLLMClient

T = TypeVar('T', bound=BaseModel)
logger = logging.getLogger(__name__)

class LLMService:
    """LLM service with automatic fallback."""
    
    def __init__(
        self,
        primary: BaseLLMClient,
        secondary: Optional[BaseLLMClient] = None
    ):
        self._primary = primary
        self._secondary = secondary
    
    async def generate_structured(
        self,
        prompt: str,
        response_model: Type[T],
        temperature: float = 0.1
    ) -> T:
        """Generate structured response with fallback on failure."""
        try:
            return await self._primary.generate_structured(
                prompt=prompt,
                response_model=response_model,
                temperature=temperature
            )
        except Exception as e:
            logger.warning(f"Primary LLM failed: {e}")
            if self._secondary:
                logger.info("Falling back to secondary LLM")
                return await self._secondary.generate_structured(
                    prompt=prompt,
                    response_model=response_model,
                    temperature=temperature
                )
            raise
```

**Verify:** Run `pytest backend/tests/scraper/test_llm_client.py -v`

---

### WIRING: Export LLM components

Update `backend/app/scraper/llm/__init__.py`:
```python
from app.scraper.llm.base import BaseLLMClient
from app.scraper.llm.gemini import GeminiClient
from app.scraper.llm.deepseek import DeepseekClient
from app.scraper.llm.service import LLMService

__all__ = ["BaseLLMClient", "GeminiClient", "DeepseekClient", "LLMService"]
```

---

## Finalize Slice 2

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 2.1 Create `BaseLLMClient` protocol
- [x] 2.2 Implement `GeminiClient`
- [x] 2.3 Implement `DeepseekClient`
- [x] 2.4 Implement `LLMService` with fallback chain
- [x] 2.5 Add instructor patching to `LLMService`
- [x] **SLICE 2 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add LLM client layer with fallback

- Add BaseLLMClient protocol
- Implement GeminiClient with instructor
- Implement DeepseekClient with instructor
- Add LLMService with automatic fallback chain"
```

---

# SLICE 3: Scraper Base Infrastructure

## Context
Now we build the reusable scraper foundation: rate limiting, retries, and User-Agent rotation.

**Dependencies:** SLICE 1 must be complete.

**Relevant Existing Files:**
- `backend/app/scraper/` - Existing scraper directory

## Prompt

You are implementing SLICE 3 of the Smart Scraper project. Follow TDD strictly.

### TASK 3.1: Rate Limiter (Test: Delays Respected)

**Test First:**
Create `backend/tests/scraper/test_base_scraper.py`:
```python
"""Test base scraper infrastructure."""
import pytest
import asyncio
from unittest.mock import AsyncMock, patch
import time

@pytest.mark.asyncio
async def test_rate_limiter_enforces_delay():
    """RateLimiter should enforce minimum delay between calls."""
    from app.scraper.base.rate_limiter import RateLimiter
    
    limiter = RateLimiter(min_delay=0.1, max_delay=0.1)  # Fixed 100ms
    
    start = time.monotonic()
    await limiter.wait()
    await limiter.wait()
    elapsed = time.monotonic() - start
    
    # Second call should have waited ~100ms
    assert elapsed >= 0.09  # Allow small timing variance

@pytest.mark.asyncio
async def test_rate_limiter_randomizes_delay():
    """RateLimiter should randomize delays within range."""
    from app.scraper.base.rate_limiter import RateLimiter
    
    limiter = RateLimiter(min_delay=0.05, max_delay=0.15)
    delays = []
    
    for _ in range(5):
        start = time.monotonic()
        await limiter.wait()
        delays.append(time.monotonic() - start)
    
    # At least some variance (not all identical)
    # First call has no delay, so check from second onwards
    assert len(set(round(d, 2) for d in delays[1:])) > 1 or len(delays) < 3
```

**Implementation:**
Create `backend/app/scraper/base/__init__.py` (empty).
Create `backend/app/scraper/base/rate_limiter.py`:
```python
"""Rate limiter for respectful scraping."""
import asyncio
import random
import time

class RateLimiter:
    """Enforces delays between requests with randomization."""
    
    def __init__(self, min_delay: float = 2.0, max_delay: float = 5.0):
        self.min_delay = min_delay
        self.max_delay = max_delay
        self._last_request: float = 0
    
    async def wait(self) -> None:
        """Wait appropriate time before next request."""
        now = time.monotonic()
        elapsed = now - self._last_request
        
        if self._last_request > 0:  # Not first request
            delay = random.uniform(self.min_delay, self.max_delay)
            if elapsed < delay:
                await asyncio.sleep(delay - elapsed)
        
        self._last_request = time.monotonic()
```

**Verify:** Run `pytest backend/tests/scraper/test_base_scraper.py -v`

---

### TASK 3.2: Retry with Backoff (Test: Retries and Backs Off)

**Test First:**
Add to `backend/tests/scraper/test_base_scraper.py`:
```python
@pytest.mark.asyncio
async def test_retry_succeeds_after_failures():
    """Retry decorator should retry on failure and succeed."""
    from app.scraper.base.retry import with_retry
    
    call_count = 0
    
    @with_retry(max_attempts=3, base_delay=0.01)
    async def flaky_function():
        nonlocal call_count
        call_count += 1
        if call_count < 3:
            raise ConnectionError("Temporary failure")
        return "success"
    
    result = await flaky_function()
    assert result == "success"
    assert call_count == 3

@pytest.mark.asyncio
async def test_retry_raises_after_max_attempts():
    """Retry decorator should raise after max attempts exceeded."""
    from app.scraper.base.retry import with_retry
    
    @with_retry(max_attempts=2, base_delay=0.01)
    async def always_fails():
        raise ConnectionError("Always fails")
    
    with pytest.raises(ConnectionError):
        await always_fails()
```

**Implementation:**
Create `backend/app/scraper/base/retry.py`:
```python
"""Retry decorator with exponential backoff."""
import asyncio
import functools
import logging
from typing import TypeVar, Callable, Any

logger = logging.getLogger(__name__)
F = TypeVar('F', bound=Callable[..., Any])

def with_retry(
    max_attempts: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exceptions: tuple = (Exception,)
) -> Callable[[F], F]:
    """Decorator for async functions with exponential backoff retry."""
    
    def decorator(func: F) -> F:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(1, max_attempts + 1):
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt == max_attempts:
                        logger.error(f"{func.__name__} failed after {max_attempts} attempts")
                        raise
                    
                    delay = min(base_delay * (2 ** (attempt - 1)), max_delay)
                    logger.warning(
                        f"{func.__name__} attempt {attempt} failed: {e}. "
                        f"Retrying in {delay:.1f}s..."
                    )
                    await asyncio.sleep(delay)
            
            raise last_exception
        
        return wrapper
    return decorator
```

**Verify:** Run `pytest backend/tests/scraper/test_base_scraper.py -v`

---

### TASK 3.3: User-Agent Rotation (Test: Headers Rotate)

**Test First:**
Add to `backend/tests/scraper/test_base_scraper.py`:
```python
def test_user_agent_rotator_returns_different_agents():
    """UserAgentRotator should return varying user agents."""
    from app.scraper.base.user_agent import UserAgentRotator
    
    rotator = UserAgentRotator()
    agents = [rotator.get() for _ in range(10)]
    
    # Should have at least 2 different agents in 10 calls
    assert len(set(agents)) >= 2

def test_user_agent_rotator_all_valid():
    """All user agents should be valid strings."""
    from app.scraper.base.user_agent import UserAgentRotator
    
    rotator = UserAgentRotator()
    for _ in range(10):
        agent = rotator.get()
        assert isinstance(agent, str)
        assert len(agent) > 20  # Reasonable UA length
        assert "Mozilla" in agent or "ChainlinesBot" in agent
```

**Implementation:**
Create `backend/app/scraper/base/user_agent.py`:
```python
"""User-Agent rotation for scraping."""
import random

# Common browser user agents
_USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "ChainlinesBot/1.0 (https://chainlines.app; contact@chainlines.app)",
]

class UserAgentRotator:
    """Rotates through user agents for requests."""
    
    def __init__(self, agents: list[str] | None = None):
        self._agents = agents or _USER_AGENTS
    
    def get(self) -> str:
        """Get a random user agent."""
        return random.choice(self._agents)
```

**Verify:** Run `pytest backend/tests/scraper/test_base_scraper.py -v`

---

### TASK 3.4: Base Scraper Class (Test: Fetches with Rate Limiting)

**Test First:**
Add to `backend/tests/scraper/test_base_scraper.py`:
```python
@pytest.mark.asyncio
async def test_base_scraper_fetches_with_rate_limit():
    """BaseScraper should fetch URLs respecting rate limits."""
    from app.scraper.base.scraper import BaseScraper
    
    class TestScraper(BaseScraper):
        pass
    
    scraper = TestScraper(min_delay=0.01, max_delay=0.01)
    
    # Mock httpx
    with patch('app.scraper.base.scraper.httpx.AsyncClient') as mock_client_class:
        mock_client = AsyncMock()
        mock_response = AsyncMock()
        mock_response.text = "<html>test</html>"
        mock_response.status_code = 200
        mock_response.raise_for_status = lambda: None
        mock_client.get = AsyncMock(return_value=mock_response)
        mock_client.__aenter__ = AsyncMock(return_value=mock_client)
        mock_client.__aexit__ = AsyncMock()
        mock_client_class.return_value = mock_client
        
        html = await scraper.fetch("https://example.com")
        assert html == "<html>test</html>"
```

**Implementation:**
Create `backend/app/scraper/base/scraper.py`:
```python
"""Base scraper with rate limiting and retries."""
import httpx
from app.scraper.base.rate_limiter import RateLimiter
from app.scraper.base.retry import with_retry
from app.scraper.base.user_agent import UserAgentRotator

class BaseScraper:
    """Base class for all scrapers."""
    
    def __init__(
        self,
        min_delay: float = 3.0,
        max_delay: float = 6.0,
        timeout: float = 30.0
    ):
        self._rate_limiter = RateLimiter(min_delay, max_delay)
        self._user_agent = UserAgentRotator()
        self._timeout = timeout
    
    @with_retry(max_attempts=3, base_delay=2.0, exceptions=(httpx.HTTPError,))
    async def fetch(self, url: str) -> str:
        """Fetch a URL with rate limiting and retries."""
        await self._rate_limiter.wait()
        
        headers = {"User-Agent": self._user_agent.get()}
        
        async with httpx.AsyncClient(timeout=self._timeout) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            return response.text
```

**Verify:** Run `pytest backend/tests/scraper/test_base_scraper.py -v`

---

### WIRING: Export base scraper components

Update `backend/app/scraper/base/__init__.py`:
```python
from app.scraper.base.scraper import BaseScraper
from app.scraper.base.rate_limiter import RateLimiter
from app.scraper.base.retry import with_retry
from app.scraper.base.user_agent import UserAgentRotator

__all__ = ["BaseScraper", "RateLimiter", "with_retry", "UserAgentRotator"]
```

---

## Finalize Slice 3

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 3.1 Create `BaseScraper` abstract class
- [x] 3.2 Implement rate limiting with configurable delays
- [x] 3.3 Implement retry with exponential backoff
- [x] 3.4 Implement User-Agent rotation
- [x] **SLICE 3 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add base scraper infrastructure

- Add RateLimiter with configurable random delays
- Add retry decorator with exponential backoff
- Add UserAgentRotator for header rotation
- Add BaseScraper class combining all features"
```

---

# SLICE 4: CyclingFlash Scraper

## Context
With the base infrastructure in place, we now implement the primary data source scraper for CyclingFlash.

**Dependencies:** SLICE 3 must be complete.

**Relevant Existing Files:**
- `backend/app/scraper/base/` - Base scraper infrastructure

## Prompt

You are implementing SLICE 4 of the Smart Scraper project. Follow TDD strictly.

### TASK 4.1: Create HTML Fixture Files

**Setup:**
Create directory `backend/tests/scraper/fixtures/cyclingflash/`

Create `backend/tests/scraper/fixtures/cyclingflash/team_list_2024.html`:
```html
<!-- Simplified fixture representing CyclingFlash team list -->
<html>
<body>
<div class="team-list">
  <a href="/team/uae-team-emirates-2024">UAE Team Emirates</a>
  <a href="/team/team-visma-lease-a-bike-2024">Team Visma | Lease a Bike</a>
  <a href="/team/soudal-quick-step-2024">Soudal Quick-Step</a>
</div>
</body>
</html>
```

Create `backend/tests/scraper/fixtures/cyclingflash/team_detail_2024.html`:
```html
<!-- Simplified fixture representing CyclingFlash team detail -->
<html>
<body>
<div class="team-header">
  <h1>Team Visma | Lease a Bike (2024)</h1>
  <span class="uci-code">TJV</span>
  <span class="tier">WorldTour</span>
  <span class="country">NED</span>
</div>
<div class="sponsors">
  <span class="sponsor">Visma</span>
  <span class="sponsor">Lease a Bike</span>
</div>
<a class="prev-season" href="/team/team-jumbo-visma-2023">Previous Season</a>
</body>
</html>
```

---

### TASK 4.2: Team List Parser (Test: Extracts Team URLs)

**Test First:**
Create `backend/tests/scraper/test_cyclingflash.py`:
```python
"""Test CyclingFlash scraper."""
import pytest
from pathlib import Path

FIXTURE_DIR = Path(__file__).parent / "fixtures" / "cyclingflash"

def test_parse_team_list_extracts_urls():
    """Parser should extract team URLs from list page."""
    from app.scraper.sources.cyclingflash import CyclingFlashParser
    
    html = (FIXTURE_DIR / "team_list_2024.html").read_text()
    parser = CyclingFlashParser()
    
    urls = parser.parse_team_list(html)
    
    assert len(urls) == 3
    assert "/team/uae-team-emirates-2024" in urls
    assert "/team/team-visma-lease-a-bike-2024" in urls
```

**Implementation:**
Create `backend/app/scraper/sources/__init__.py` (empty).
Create `backend/app/scraper/sources/cyclingflash.py`:
```python
"""CyclingFlash scraper implementation."""
import re
from typing import Optional
from bs4 import BeautifulSoup
from pydantic import BaseModel, Field
from app.scraper.base import BaseScraper

class ScrapedTeamData(BaseModel):
    """Data extracted from a team page."""
    name: str
    uci_code: Optional[str] = None
    tier: Optional[str] = None
    country_code: Optional[str] = Field(
        default=None,
        description="3-letter IOC/UCI country code (e.g., NED, GER, ITA, FRA)"
    )
    sponsors: list[str] = []
    previous_season_url: Optional[str] = None
    season_year: int

class CyclingFlashParser:
    """Parser for CyclingFlash HTML."""
    
    def parse_team_list(self, html: str) -> list[str]:
        """Extract team URLs from list page."""
        soup = BeautifulSoup(html, 'html.parser')
        urls = []
        
        for link in soup.select('.team-list a'):
            href = link.get('href')
            if href and '/team/' in href:
                urls.append(href)
        
        return urls
```

**Verify:** Run `pytest backend/tests/scraper/test_cyclingflash.py::test_parse_team_list_extracts_urls -v`

---

### TASK 4.3: Team Detail Parser (Test: Extracts Team Data)

**Test First:**
Add to `backend/tests/scraper/test_cyclingflash.py`:
```python
def test_parse_team_detail_extracts_data():
    """Parser should extract full team data from detail page."""
    from app.scraper.sources.cyclingflash import CyclingFlashParser
    
    html = (FIXTURE_DIR / "team_detail_2024.html").read_text()
    parser = CyclingFlashParser()
    
    data = parser.parse_team_detail(html, season_year=2024)
    
    assert data.name == "Team Visma | Lease a Bike"
    assert data.uci_code == "TJV"
    assert data.tier == "WorldTour"
    assert data.country_code == "NED"
    assert data.sponsors == ["Visma", "Lease a Bike"]
    assert data.previous_season_url == "/team/team-jumbo-visma-2023"
    assert data.season_year == 2024
```

**Implementation:**
Add to `backend/app/scraper/sources/cyclingflash.py`:
```python
# Add to CyclingFlashParser class:

def parse_team_detail(self, html: str, season_year: int) -> ScrapedTeamData:
    """Extract team data from detail page."""
    soup = BeautifulSoup(html, 'html.parser')
    
    # Extract name (remove year suffix)
    header = soup.select_one('.team-header h1')
    raw_name = header.get_text(strip=True) if header else "Unknown"
    name = re.sub(r'\s*\(\d{4}\)\s*$', '', raw_name)
    
    # Extract fields
    uci_code = self._get_text(soup, '.uci-code')
    tier = self._get_text(soup, '.tier')
    country_code = self._get_text(soup, '.country')
    
    # Extract sponsors
    sponsors = [s.get_text(strip=True) for s in soup.select('.sponsors .sponsor')]
    
    # Extract previous season link
    prev_link = soup.select_one('.prev-season')
    prev_url = prev_link.get('href') if prev_link else None
    
    return ScrapedTeamData(
        name=name,
        uci_code=uci_code,
        tier=tier,
        country_code=country_code,
        sponsors=sponsors,
        previous_season_url=prev_url,
        season_year=season_year
    )

def _get_text(self, soup: BeautifulSoup, selector: str) -> Optional[str]:
    """Safely extract text from selector."""
    elem = soup.select_one(selector)
    return elem.get_text(strip=True) if elem else None
```

**Verify:** Run `pytest backend/tests/scraper/test_cyclingflash.py -v`

---

### TASK 4.4: CyclingFlash Scraper Class (Test: Integration)

**Test First:**
Add to `backend/tests/scraper/test_cyclingflash.py`:
```python
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_cyclingflash_scraper_gets_team():
    """CyclingFlashScraper should fetch and parse team data."""
    from app.scraper.sources.cyclingflash import CyclingFlashScraper
    
    html = (FIXTURE_DIR / "team_detail_2024.html").read_text()
    
    with patch.object(CyclingFlashScraper, 'fetch', new_callable=AsyncMock) as mock_fetch:
        mock_fetch.return_value = html
        
        scraper = CyclingFlashScraper(min_delay=0, max_delay=0)
        data = await scraper.get_team("/team/test-2024", season_year=2024)
        
        assert data.name == "Team Visma | Lease a Bike"
        mock_fetch.assert_called_once()
```

**Implementation:**
Add to `backend/app/scraper/sources/cyclingflash.py`:
```python
class CyclingFlashScraper(BaseScraper):
    """Scraper for CyclingFlash website."""
    
    BASE_URL = "https://cyclingflash.com"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._parser = CyclingFlashParser()
    
    async def get_team_list(self, year: int) -> list[str]:
        """Get list of team URLs for a given year."""
        url = f"{self.BASE_URL}/teams/{year}"
        html = await self.fetch(url)
        return self._parser.parse_team_list(html)
    
    async def get_team(self, path: str, season_year: int) -> ScrapedTeamData:
        """Get team details from a team page."""
        url = f"{self.BASE_URL}{path}" if not path.startswith("http") else path
        html = await self.fetch(url)
        return self._parser.parse_team_detail(html, season_year)
```

**Verify:** Run `pytest backend/tests/scraper/test_cyclingflash.py -v`

---

### WIRING: Export CyclingFlash components

Update `backend/app/scraper/sources/__init__.py`:
```python
from app.scraper.sources.cyclingflash import (
    CyclingFlashScraper,
    CyclingFlashParser,
    ScrapedTeamData
)

__all__ = ["CyclingFlashScraper", "CyclingFlashParser", "ScrapedTeamData"]
```

---

## Finalize Slice 4

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 4.1 Create HTML fixture files
- [x] 4.2 Implement team list parser
- [x] 4.3 Implement team detail parser
- [x] 4.4 Implement "Previous Season" link follower
- [x] 4.5 Integrate into `CyclingFlashScraper` class
- [x] **SLICE 4 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add CyclingFlash scraper

- Add HTML fixtures for testing
- Implement CyclingFlashParser for team list and detail pages
- Implement CyclingFlashScraper extending BaseScraper
- Add ScrapedTeamData Pydantic model"
```

---

# SLICE 5: LLM Prompts - Data Extraction

## Context
Now we connect the LLM layer to the scraper layer, enabling structured data extraction from HTML.

**Dependencies:** SLICE 2 and SLICE 4 must be complete.

## Prompt

You are implementing SLICE 5 of the Smart Scraper project. Follow TDD strictly.

### TASK 5.1: Enhance ScrapedTeamData Model

**Test First:**
Create `backend/tests/scraper/test_llm_prompts.py`:
```python
"""Test LLM prompts for data extraction."""
import pytest
from pydantic import ValidationError

def test_scraped_team_data_validates():
    """ScrapedTeamData should validate correctly."""
    from app.scraper.sources.cyclingflash import ScrapedTeamData
    
    data = ScrapedTeamData(
        name="Team Visma",
        season_year=2024,
        sponsors=["Visma", "Lease a Bike"]
    )
    assert data.name == "Team Visma"
    assert len(data.sponsors) == 2

def test_scraped_team_data_requires_name():
    """ScrapedTeamData should require name field."""
    from app.scraper.sources.cyclingflash import ScrapedTeamData
    
    with pytest.raises(ValidationError):
        ScrapedTeamData(season_year=2024)
```

**Verify:** Run `pytest backend/tests/scraper/test_llm_prompts.py -v`

---

### TASK 5.2: Team Data Extraction Prompt

**Test First:**
Add to `backend/tests/scraper/test_llm_prompts.py`:
```python
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_extract_team_data_prompt():
    """LLMService.extract_team_data should return structured data."""
    from app.scraper.llm.prompts import ScraperPrompts
    from app.scraper.sources.cyclingflash import ScrapedTeamData
    
    mock_service = AsyncMock()
    mock_service.generate_structured = AsyncMock(
        return_value=ScrapedTeamData(
            name="UAE Team Emirates",
            uci_code="UAD",
            tier="WorldTour",
            country_code="UAE",
            sponsors=["Emirates", "Colnago"],
            season_year=2024
        )
    )
    
    prompts = ScraperPrompts(llm_service=mock_service)
    
    result = await prompts.extract_team_data(
        html="<html>...</html>",
        season_year=2024
    )
    
    assert result.name == "UAE Team Emirates"
    assert result.uci_code == "UAD"
    mock_service.generate_structured.assert_called_once()
```

**Implementation:**
Create `backend/app/scraper/llm/prompts.py`:
```python
"""LLM prompts for scraper operations."""
from typing import TYPE_CHECKING
from app.scraper.sources.cyclingflash import ScrapedTeamData

if TYPE_CHECKING:
    from app.scraper.llm.service import LLMService

EXTRACT_TEAM_DATA_PROMPT = """
Analyze the following HTML from a cycling team page and extract structured data.

HTML Content:
{html}

Season Year: {season_year}

Extract the following information:
- Team name (without year suffix)
- UCI code (3-letter code if present)
- Tier level (WorldTour, ProTeam, Continental, or null)
- Country code (3-letter IOC/UCI code, e.g., NED, GER, FRA, if determinable)
- List of sponsor names (in order of appearance/prominence)
- Previous season URL (if there's a link to previous year's page)

Return the data in the specified JSON format.
"""

class ScraperPrompts:
    """Collection of LLM prompts for scraper operations."""
    
    def __init__(self, llm_service: "LLMService"):
        self._llm = llm_service
    
    async def extract_team_data(
        self,
        html: str,
        season_year: int
    ) -> ScrapedTeamData:
        """Extract structured team data from HTML using LLM."""
        prompt = EXTRACT_TEAM_DATA_PROMPT.format(
            html=html[:10000],  # Limit HTML size
            season_year=season_year
        )
        
        return await self._llm.generate_structured(
            prompt=prompt,
            response_model=ScrapedTeamData
        )
```

**Verify:** Run `pytest backend/tests/scraper/test_llm_prompts.py -v`

---

### WIRING: Export prompts

Update `backend/app/scraper/llm/__init__.py`:
```python
from app.scraper.llm.base import BaseLLMClient
from app.scraper.llm.gemini import GeminiClient
from app.scraper.llm.deepseek import DeepseekClient
from app.scraper.llm.service import LLMService
from app.scraper.llm.prompts import ScraperPrompts

__all__ = [
    "BaseLLMClient", "GeminiClient", "DeepseekClient", 
    "LLMService", "ScraperPrompts"
]
```

---

## Finalize Slice 5

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 5.1 Define `ScrapedTeamData` Pydantic model
- [x] 5.2 Write `extract_team_data` prompt
- [x] 5.3 Integrate prompt into `LLMService`
- [x] 5.4 Test with CyclingFlash fixture HTML
- [x] **SLICE 5 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add team data extraction LLM prompt

- Add ScraperPrompts class for LLM prompt management
- Implement extract_team_data prompt with structured output
- Wire ScraperPrompts to LLMService"
```

---

# SLICE 6: Checkpoint System

## Context
To enable safe resume after failures, we implement a checkpointing system.

**Dependencies:** SLICE 1 must be complete.

## Prompt

You are implementing SLICE 6 of the Smart Scraper project. Follow TDD strictly.

### TASK 6.1: Checkpoint Schema

**Test First:**
Create `backend/tests/scraper/test_checkpoint.py`:
```python
"""Test checkpoint system."""
import pytest
import json
from pathlib import Path
import tempfile

def test_checkpoint_schema_validates():
    """CheckpointData should validate correctly."""
    from app.scraper.checkpoint import CheckpointData
    
    cp = CheckpointData(
        phase=1,
        current_position="https://example.com/team/1",
        completed_urls=["https://example.com/team/2"],
        sponsor_names={"Visma", "Jumbo"}
    )
    
    assert cp.phase == 1
    assert len(cp.completed_urls) == 1
    assert "Visma" in cp.sponsor_names
```

**Implementation:**
Create `backend/app/scraper/checkpoint.py`:
```python
"""Checkpoint system for scraper resume capability."""
import json
from datetime import datetime
from pathlib import Path
from typing import Optional
from pydantic import BaseModel, Field

class CheckpointData(BaseModel):
    """Data stored in a checkpoint."""
    phase: int = 1
    current_position: Optional[str] = None
    completed_urls: list[str] = Field(default_factory=list)
    sponsor_names: set[str] = Field(default_factory=set)
    team_queue: list[str] = Field(default_factory=list)
    last_updated: datetime = Field(default_factory=datetime.utcnow)
    
    class Config:
        json_encoders = {
            set: list,  # JSON doesn't support sets
            datetime: lambda v: v.isoformat()
        }
```

**Verify:** Run `pytest backend/tests/scraper/test_checkpoint.py -v`

---

### TASK 6.2: Checkpoint Manager

**Test First:**
Add to `backend/tests/scraper/test_checkpoint.py`:
```python
def test_checkpoint_manager_save_and_load():
    """CheckpointManager should save and load checkpoints."""
    from app.scraper.checkpoint import CheckpointManager, CheckpointData
    
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "checkpoint.json"
        manager = CheckpointManager(path)
        
        # Save
        data = CheckpointData(phase=2, current_position="test-url")
        manager.save(data)
        
        # Load
        loaded = manager.load()
        assert loaded is not None
        assert loaded.phase == 2
        assert loaded.current_position == "test-url"

def test_checkpoint_manager_returns_none_if_no_file():
    """CheckpointManager.load should return None if no checkpoint exists."""
    from app.scraper.checkpoint import CheckpointManager
    
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "nonexistent.json"
        manager = CheckpointManager(path)
        
        assert manager.load() is None

def test_checkpoint_manager_clear():
    """CheckpointManager.clear should delete checkpoint file."""
    from app.scraper.checkpoint import CheckpointManager, CheckpointData
    
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "checkpoint.json"
        manager = CheckpointManager(path)
        
        manager.save(CheckpointData())
        assert path.exists()
        
        manager.clear()
        assert not path.exists()
```

**Implementation:**
Add to `backend/app/scraper/checkpoint.py`:
```python
class CheckpointManager:
    """Manages checkpoint persistence."""
    
    def __init__(self, path: Path):
        self._path = path
    
    def save(self, data: CheckpointData) -> None:
        """Save checkpoint to file."""
        self._path.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert for JSON serialization
        json_data = data.model_dump()
        json_data['sponsor_names'] = list(json_data['sponsor_names'])
        json_data['last_updated'] = data.last_updated.isoformat()
        
        self._path.write_text(json.dumps(json_data, indent=2))
    
    def load(self) -> Optional[CheckpointData]:
        """Load checkpoint from file, or None if not exists."""
        if not self._path.exists():
            return None
        
        json_data = json.loads(self._path.read_text())
        json_data['sponsor_names'] = set(json_data['sponsor_names'])
        json_data['last_updated'] = datetime.fromisoformat(json_data['last_updated'])
        
        return CheckpointData(**json_data)
    
    def clear(self) -> None:
        """Delete checkpoint file."""
        if self._path.exists():
            self._path.unlink()
```

**Verify:** Run `pytest backend/tests/scraper/test_checkpoint.py -v`

---

### WIRING: Export checkpoint components

The checkpoint module is already self-contained. Update imports:
```python
# In backend/app/scraper/__init__.py (create if needed):
from app.scraper.checkpoint import CheckpointManager, CheckpointData
```

---

## Finalize Slice 6

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 6.1 Design checkpoint JSON schema
- [x] 6.2 Implement `CheckpointManager` class
- [x] 6.3 Add checkpoint file location config
- [x] 6.4 Integrate with scraper loop
- [x] **SLICE 6 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add checkpoint/resume system

- Add CheckpointData Pydantic model
- Add CheckpointManager for save/load/clear operations
- Support JSON persistence with set/datetime handling"
```

---
# SLICE 7: Phase 1 Orchestration - Discovery

## Context
With scrapers and LLM layer ready, we now build Phase 1 orchestration: spidering CyclingFlash to discover teams and collect sponsor names.

**Dependencies:** SLICES 4, 5, and 6 must be complete.

**Relevant Existing Files:**
- `backend/app/scraper/sources/cyclingflash.py` - CyclingFlash scraper
- `backend/app/scraper/llm/prompts.py` - LLM prompts
- `backend/app/scraper/checkpoint.py` - Checkpoint system

## Prompt

You are implementing SLICE 7 of the Smart Scraper project. Follow TDD strictly.

### TASK 7.1: Sponsor Name Collector (Test: Extracts Unique Names)

**Test First:**
Create `backend/tests/scraper/test_phase1.py`:
```python
"""Test Phase 1 Discovery orchestration."""
import pytest
from unittest.mock import AsyncMock, MagicMock

def test_sponsor_collector_extracts_unique():
    """SponsorCollector should collect unique sponsor names."""
    from app.scraper.orchestration.phase1 import SponsorCollector
    
    collector = SponsorCollector()
    
    collector.add(["Visma", "Lease a Bike"])
    collector.add(["Visma", "Jumbo"])  # Visma duplicate
    
    assert len(collector.get_all()) == 3
    assert "Visma" in collector.get_all()
    assert "Jumbo" in collector.get_all()
```

**Implementation:**
Create `backend/app/scraper/orchestration/__init__.py` (empty).
Create `backend/app/scraper/orchestration/phase1.py`:
```python
"""Phase 1: Discovery and Sponsor Collection."""
from typing import Set, List

class SponsorCollector:
    """Collects unique sponsor names across all teams."""
    
    def __init__(self):
        self._names: Set[str] = set()
    
    def add(self, sponsors: List[str]) -> None:
        """Add sponsor names to collection."""
        for name in sponsors:
            if name and name.strip():
                self._names.add(name.strip())
    
    def get_all(self) -> Set[str]:
        """Get all unique sponsor names."""
        return self._names.copy()
```

**Verify:** Run `pytest backend/tests/scraper/test_phase1.py -v`

---

### TASK 7.2: Discovery Service (Test: Spiders Teams)

**Test First:**
Add to `backend/tests/scraper/test_phase1.py`:
```python
@pytest.mark.asyncio
async def test_discovery_service_collects_teams():
    """DiscoveryService should collect team URLs by spidering."""
    from app.scraper.orchestration.phase1 import DiscoveryService
    from app.scraper.sources.cyclingflash import ScrapedTeamData
    
    mock_scraper = AsyncMock()
    mock_scraper.get_team_list = AsyncMock(return_value=["/team/a", "/team/b"])
    mock_scraper.get_team = AsyncMock(return_value=ScrapedTeamData(
        name="Team A",
        season_year=2024,
        sponsors=["Sponsor1"],
        previous_season_url=None
    ))
    
    mock_checkpoint = MagicMock()
    mock_checkpoint.load.return_value = None
    
    service = DiscoveryService(
        scraper=mock_scraper,
        checkpoint_manager=mock_checkpoint
    )
    
    result = await service.discover_teams(start_year=2024, end_year=2024)
    
    assert len(result.team_urls) >= 2
    assert "Sponsor1" in result.sponsor_names
```

**Implementation:**
Add to `backend/app/scraper/orchestration/phase1.py`:
```python
import logging
from dataclasses import dataclass
from app.scraper.sources.cyclingflash import CyclingFlashScraper
from app.scraper.checkpoint import CheckpointManager, CheckpointData

logger = logging.getLogger(__name__)

@dataclass
class DiscoveryResult:
    """Result of Phase 1 discovery."""
    team_urls: list[str]
    sponsor_names: set[str]

class DiscoveryService:
    """Orchestrates Phase 1: Team discovery and sponsor collection."""
    
    def __init__(
        self,
        scraper: CyclingFlashScraper,
        checkpoint_manager: CheckpointManager
    ):
        self._scraper = scraper
        self._checkpoint = checkpoint_manager
        self._collector = SponsorCollector()
    
    async def discover_teams(
        self,
        start_year: int,
        end_year: int
    ) -> DiscoveryResult:
        """Discover all teams and collect sponsor names."""
        checkpoint = self._checkpoint.load()
        team_urls: list[str] = []
        
        if checkpoint and checkpoint.phase == 1:
            team_urls = checkpoint.team_queue.copy()
            self._collector._names = checkpoint.sponsor_names.copy()
            logger.info(f"Resuming from checkpoint with {len(team_urls)} teams")
        
        for year in range(start_year, end_year - 1, -1):  # Backwards
            try:
                urls = await self._scraper.get_team_list(year)
                for url in urls:
                    if url not in team_urls:
                        team_urls.append(url)
                        # Get team details for sponsors
                        data = await self._scraper.get_team(url, year)
                        self._collector.add(data.sponsors)
                        
                        # Save checkpoint periodically
                        self._save_checkpoint(team_urls)
                        
            except Exception as e:
                logger.error(f"Error in year {year}: {e}")
                self._save_checkpoint(team_urls)
                raise
        
        return DiscoveryResult(
            team_urls=team_urls,
            sponsor_names=self._collector.get_all()
        )
    
    def _save_checkpoint(self, team_urls: list[str]) -> None:
        """Save current progress."""
        self._checkpoint.save(CheckpointData(
            phase=1,
            team_queue=team_urls,
            sponsor_names=self._collector.get_all()
        ))
```

**Verify:** Run `pytest backend/tests/scraper/test_phase1.py -v`

---

### TASK 7.3: Sponsor Resolution Model (Test: Pydantic Validation)

**Test First:**
Add to `backend/tests/scraper/test_phase1.py`:
```python
def test_sponsor_resolution_model():
    """SponsorResolution should validate correctly."""
    from app.scraper.orchestration.phase1 import SponsorResolution
    
    resolution = SponsorResolution(
        raw_name="AG2R PrÃ©voyance",
        master_name="AG2R Group",
        brand_name="AG2R PrÃ©voyance",
        hex_color="#004A9C",
        confidence=0.95
    )
    
    assert resolution.master_name == "AG2R Group"
    assert resolution.confidence >= 0.9
```

**Implementation:**
Add to `backend/app/scraper/orchestration/phase1.py`:
```python
from pydantic import BaseModel
from typing import Optional

class SponsorResolution(BaseModel):
    """LLM-resolved sponsor information."""
    raw_name: str
    master_name: str
    brand_name: str
    hex_color: str
    confidence: float
    reasoning: Optional[str] = None
```

**Verify:** Run `pytest backend/tests/scraper/test_phase1.py -v`

---

### WIRING: Export Phase 1 components

Update `backend/app/scraper/orchestration/__init__.py`:
```python
from app.scraper.orchestration.phase1 import (
    SponsorCollector,
    DiscoveryService,
    DiscoveryResult,
    SponsorResolution
)

__all__ = [
    "SponsorCollector", "DiscoveryService", 
    "DiscoveryResult", "SponsorResolution"
]
```

---

## Finalize Slice 7

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 7.1 Implement sponsor name collector
- [x] 7.2 Define `SponsorResolution` Pydantic model
- [x] 7.3 Write `resolve_sponsor` LLM prompt
- [x] 7.4 Implement brand color search logic
- [x] 7.5 Wire Phase 1 into `SmartScraperService`
- [x] **SLICE 7 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add Phase 1 discovery orchestration

- Add SponsorCollector for unique name tracking
- Add DiscoveryService for team spidering
- Add SponsorResolution Pydantic model
- Integrate with checkpoint system"
```

---

# SLICE 8: Phase 2 Orchestration - Team Assembly

## Context
Phase 2 processes the discovered teams, creates TeamNodes/TeamEras, and links sponsors with prominence calculation.

**Dependencies:** SLICE 7 must be complete.

## Prompt

You are implementing SLICE 8 of the Smart Scraper project. Follow TDD strictly.

### TASK 8.1: Prominence Calculator (Test: All Rules)

**Test First:**
Create `backend/tests/scraper/test_phase2.py`:
```python
"""Test Phase 2 Team Assembly orchestration."""
import pytest

def test_prominence_calculator_one_sponsor():
    """One sponsor should get 100%."""
    from app.scraper.orchestration.phase2 import ProminenceCalculator
    
    result = ProminenceCalculator.calculate(["Visma"])
    assert result == [100]

def test_prominence_calculator_two_sponsors():
    """Two sponsors should get 60/40."""
    from app.scraper.orchestration.phase2 import ProminenceCalculator
    
    result = ProminenceCalculator.calculate(["Visma", "Lease a Bike"])
    assert result == [60, 40]

def test_prominence_calculator_three_sponsors():
    """Three sponsors should get 40/30/30."""
    from app.scraper.orchestration.phase2 import ProminenceCalculator
    
    result = ProminenceCalculator.calculate(["A", "B", "C"])
    assert result == [40, 30, 30]

def test_prominence_calculator_four_sponsors():
    """Four sponsors should get 40/20/20/20."""
    from app.scraper.orchestration.phase2 import ProminenceCalculator
    
    result = ProminenceCalculator.calculate(["A", "B", "C", "D"])
    assert result == [40, 20, 20, 20]

def test_prominence_calculator_five_sponsors():
    """Five+ sponsors: LLM pattern extension (sum=100)."""
    from app.scraper.orchestration.phase2 import ProminenceCalculator
    
    result = ProminenceCalculator.calculate(["A", "B", "C", "D", "E"])
    assert sum(result) == 100
    assert result[0] >= result[-1]  # First should be highest
```

**Implementation:**
Create `backend/app/scraper/orchestration/phase2.py`:
```python
"""Phase 2: Team Node Assembly."""
from typing import List

class ProminenceCalculator:
    """Calculates sponsor prominence percentages."""
    
    RULES = {
        1: [100],
        2: [60, 40],
        3: [40, 30, 30],
        4: [40, 20, 20, 20],
    }
    
    @classmethod
    def calculate(cls, sponsors: List[str]) -> List[int]:
        """Calculate prominence for each sponsor."""
        count = len(sponsors)
        
        if count == 0:
            return []
        
        if count in cls.RULES:
            return cls.RULES[count]
        
        # 5+ sponsors: extend pattern (first gets 40, rest split evenly)
        first = 40
        remaining = 100 - first
        each = remaining // (count - 1)
        last_adjustment = remaining - (each * (count - 1))
        
        result = [first] + [each] * (count - 2) + [each + last_adjustment]
        return result
```

**Verify:** Run `pytest backend/tests/scraper/test_phase2.py -v`

---

### TASK 8.2: Team Assembly Service (Test: Creates via AuditLog)

**Test First:**
Add to `backend/tests/scraper/test_phase2.py`:
```python
from unittest.mock import AsyncMock, MagicMock
from uuid import uuid4

@pytest.mark.asyncio
async def test_team_assembly_creates_edit():
    """TeamAssemblyService should create edits via AuditLog."""
    from app.scraper.orchestration.phase2 import TeamAssemblyService
    from app.scraper.sources.cyclingflash import ScrapedTeamData
    
    mock_audit = AsyncMock()
    mock_audit.create_edit = AsyncMock(return_value=MagicMock(edit_id=uuid4()))
    
    mock_session = AsyncMock()
    
    service = TeamAssemblyService(
        audit_service=mock_audit,
        session=mock_session,
        system_user_id=uuid4()
    )
    
    team_data = ScrapedTeamData(
        name="Team Visma",
        season_year=2024,
        sponsors=["Visma", "Lease a Bike"],
        uci_code="TJV",
        tier="WorldTour"
    )
    
    await service.create_team_era(team_data, confidence=0.95)
    
    mock_audit.create_edit.assert_called_once()
```

**Implementation:**
Add to `backend/app/scraper/orchestration/phase2.py`:
```python
import logging
from uuid import UUID
from datetime import date
from sqlalchemy.ext.asyncio import AsyncSession
from app.scraper.sources.cyclingflash import ScrapedTeamData
from app.services.audit_log_service import AuditLogService
from app.models.enums import EditAction, EditStatus

logger = logging.getLogger(__name__)

CONFIDENCE_THRESHOLD = 0.90

class TeamAssemblyService:
    """Orchestrates Phase 2: Team and Era creation."""
    
    def __init__(
        self,
        audit_service: AuditLogService,
        session: AsyncSession,
        system_user_id: UUID
    ):
        self._audit = audit_service
        self._session = session
        self._user_id = system_user_id
    
    async def create_team_era(
        self,
        data: ScrapedTeamData,
        confidence: float
    ) -> None:
        """Create TeamNode/TeamEra via AuditLog."""
        status = (
            EditStatus.APPROVED if confidence >= CONFIDENCE_THRESHOLD
            else EditStatus.PENDING
        )
        
        # Build the edit payload
        new_data = {
            "registered_name": data.name,
            "season_year": data.season_year,
            "uci_code": data.uci_code,
            "tier_level": self._parse_tier(data.tier),
            "valid_from": f"{data.season_year}-01-01",
            "sponsors": [
                {"name": s, "prominence": p}
                for s, p in zip(
                    data.sponsors,
                    ProminenceCalculator.calculate(data.sponsors)
                )
            ]
        }
        
        await self._audit.create_edit(
            session=self._session,
            user_id=self._user_id,
            entity_type="TeamEra",
            entity_id=None,  # New entity
            action=EditAction.CREATE,
            old_data=None,
            new_data=new_data,
            status=status
        )
        
        logger.info(f"Created edit for {data.name} ({status.value})")
    
    def _parse_tier(self, tier: str | None) -> int | None:
        """Convert tier string to level."""
        if not tier:
            return None
        tier_map = {"WorldTour": 1, "ProTeam": 2, "Continental": 3}
        return tier_map.get(tier)
```

**Verify:** Run `pytest backend/tests/scraper/test_phase2.py -v`

---

### WIRING: Export Phase 2 components

Update `backend/app/scraper/orchestration/__init__.py`:
```python
from app.scraper.orchestration.phase1 import (
    SponsorCollector, DiscoveryService, 
    DiscoveryResult, SponsorResolution
)
from app.scraper.orchestration.phase2 import (
    ProminenceCalculator, TeamAssemblyService
)

__all__ = [
    "SponsorCollector", "DiscoveryService", 
    "DiscoveryResult", "SponsorResolution",
    "ProminenceCalculator", "TeamAssemblyService"
]
```

---

## Finalize Slice 8

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 8.1 Implement team queue processor
- [x] 8.2 Implement sponsor mapping (string â†’ UUID)
- [x] 8.3 Implement prominence calculation
- [x] 8.4 Integrate with AuditLogService for writes
- [x] 8.5 Wire Phase 2 into `SmartScraperService`
- [x] **SLICE 8 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add Phase 2 team assembly orchestration

- Add ProminenceCalculator with all prominence rules
- Add TeamAssemblyService integrating with AuditLogService
- Support auto-approve (>=90%) and pending (<90%) edits"
```

---


# SLICE 9: LLM Prompts - Lineage Decision

## Context
We need the LLM to determine lineage relationships between teams (transfers, merges, splits, spiritual succession).

**Dependencies:** SLICE 2 must be complete.

## Prompt

You are implementing SLICE 9 of the Smart Scraper project. Follow TDD strictly.

### TASK 9.1: LineageDecision Model (Test: All Event Types)

**Test First:**
Create `backend/tests/scraper/test_lineage_prompt.py`:
```python
"""Test Lineage Decision LLM prompts."""
import pytest
from uuid import uuid4

def test_lineage_decision_validates():
    """LineageDecision should validate correctly."""
    from app.scraper.llm.lineage import LineageDecision
    from app.models.enums import LineageEventType
    
    decision = LineageDecision(
        event_type=LineageEventType.LEGAL_TRANSFER,
        confidence=0.95,
        reasoning="Same UCI code, continuous license",
        predecessor_ids=[uuid4()],
        successor_ids=[uuid4()]
    )
    
    assert decision.event_type == LineageEventType.LEGAL_TRANSFER
    assert decision.confidence >= 0.9

def test_lineage_decision_merge_has_multiple_predecessors():
    """MERGE should allow multiple predecessors."""
    from app.scraper.llm.lineage import LineageDecision
    from app.models.enums import LineageEventType
    
    decision = LineageDecision(
        event_type=LineageEventType.MERGE,
        confidence=0.85,
        reasoning="Two teams combined",
        predecessor_ids=[uuid4(), uuid4()],
        successor_ids=[uuid4()]
    )
    
    assert len(decision.predecessor_ids) == 2
    assert len(decision.successor_ids) == 1

def test_lineage_decision_split_has_multiple_successors():
    """SPLIT should allow multiple successors."""
    from app.scraper.llm.lineage import LineageDecision
    from app.models.enums import LineageEventType
    
    decision = LineageDecision(
        event_type=LineageEventType.SPLIT,
        confidence=0.80,
        reasoning="Team dissolved into two",
        predecessor_ids=[uuid4()],
        successor_ids=[uuid4(), uuid4()]
    )
    
    assert len(decision.predecessor_ids) == 1
    assert len(decision.successor_ids) == 2
```

**Implementation:**
Create `backend/app/scraper/llm/lineage.py`:
```python
"""Lineage decision models and prompts."""
from typing import List, Optional
from uuid import UUID
from pydantic import BaseModel
from app.models.enums import LineageEventType

class LineageDecision(BaseModel):
    """LLM decision about team lineage."""
    event_type: LineageEventType
    confidence: float
    reasoning: str
    predecessor_ids: List[UUID]
    successor_ids: List[UUID]
    notes: Optional[str] = None
```

**Verify:** Run `pytest backend/tests/scraper/test_lineage_prompt.py -v`

---

### TASK 9.2: Lineage Decision Prompt

**Test First:**
Add to `backend/tests/scraper/test_lineage_prompt.py`:
```python
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_decide_lineage_prompt():
    """ScraperPrompts.decide_lineage should return decision."""
    from app.scraper.llm.prompts import ScraperPrompts
    from app.scraper.llm.lineage import LineageDecision
    from app.models.enums import LineageEventType
    
    mock_service = AsyncMock()
    mock_service.generate_structured = AsyncMock(
        return_value=LineageDecision(
            event_type=LineageEventType.LEGAL_TRANSFER,
            confidence=0.92,
            reasoning="Continuation of same team",
            predecessor_ids=[uuid4()],
            successor_ids=[uuid4()]
        )
    )
    
    prompts = ScraperPrompts(llm_service=mock_service)
    
    result = await prompts.decide_lineage(
        predecessor_info="Team A ended 2023, UCI code TJV",
        successor_info="Team B started 2024, UCI code TJV"
    )
    
    assert result.event_type == LineageEventType.LEGAL_TRANSFER
    mock_service.generate_structured.assert_called_once()
```

**Implementation:**
Add to `backend/app/scraper/llm/prompts.py`:
```python
from app.scraper.llm.lineage import LineageDecision

DECIDE_LINEAGE_PROMPT = """
Analyze the relationship between these cycling teams and determine the lineage type.

PREDECESSOR TEAM:
{predecessor_info}

SUCCESSOR TEAM:
{successor_info}

Determine the relationship type:
- LEGAL_TRANSFER: Same legal entity, continuous UCI license (the standard season-to-season continuation)
- SPIRITUAL_SUCCESSION: No legal link, but cultural/personnel continuity (often documented in Wikipedia "History" sections)
- MERGE: Multiple predecessors combined into one successor (includes joins into an already-existing team)
- SPLIT: One predecessor split into multiple successors (includes spin-offs where the original team continues)

Key considerations:
- UCI codes: Same code = likely legal transfer
- Staff continuity: >50% retained staff = strong connection
- Sponsor continuity: Same major sponsors suggest legal transfer
- Wikipedia "History" sections are the best source for spiritual succession evidence

IMPORTANT: Lineage events occur on a single date (typically season start). Time gaps should be minimal
(e.g., a team folding mid-season may have a successor starting the following season).

Return your decision with confidence score (0.0 to 1.0).
"""

# Add to ScraperPrompts class:
async def decide_lineage(
    self,
    predecessor_info: str,
    successor_info: str
) -> LineageDecision:
    """Decide lineage relationship between teams."""
    prompt = DECIDE_LINEAGE_PROMPT.format(
        predecessor_info=predecessor_info,
        successor_info=successor_info
    )
    
    return await self._llm.generate_structured(
        prompt=prompt,
        response_model=LineageDecision
    )
```

**Verify:** Run `pytest backend/tests/scraper/test_lineage_prompt.py -v`

---

### WIRING: Export lineage components

Update `backend/app/scraper/llm/__init__.py`:
```python
from app.scraper.llm.lineage import LineageDecision
# ... existing exports ...

__all__ = [
    "BaseLLMClient", "GeminiClient", "DeepseekClient", 
    "LLMService", "ScraperPrompts", "LineageDecision"
]
```

---

## Finalize Slice 9

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 9.1 Define `LineageDecision` Pydantic model
- [x] 9.2 Write `decide_lineage` prompt
- [x] 9.3 Test all event types
- [x] 9.4 Test confidence scoring
- [x] **SLICE 9 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add lineage decision LLM prompt

- Add LineageDecision Pydantic model
- Add decide_lineage prompt for all event types
- Support LEGAL_TRANSFER, MERGE, SPLIT, SPIRITUAL_SUCCESSION"
```

---

# SLICE 10: Phase 3 Orchestration - Lineage Connection

## Context
Phase 3 detects orphan nodes and connects them via LineageEvents.

**Dependencies:** SLICES 8 and 9 must be complete.

## Prompt

You are implementing SLICE 10 of the Smart Scraper project. Follow TDD strictly.

### TASK 10.1: Orphan Detector (Test: Finds Gaps)

**Test First:**
Create `backend/tests/scraper/test_phase3.py`:
```python
"""Test Phase 3 Lineage Connection orchestration."""
import pytest
from datetime import date

def test_orphan_detector_finds_gaps():
    """OrphanDetector should find teams with year gaps."""
    from app.scraper.orchestration.phase3 import OrphanDetector
    
    teams = [
        {"node_id": "a", "name": "Team A", "end_year": 2022},
        {"node_id": "b", "name": "Team B", "start_year": 2023},
        {"node_id": "c", "name": "Team C", "end_year": 2020},
    ]
    
    detector = OrphanDetector()
    candidates = detector.find_candidates(teams)
    
    # Should match Team A (ended 2022) with Team B (started 2023)
    assert len(candidates) >= 1
    assert any(c["predecessor"]["name"] == "Team A" for c in candidates)

def test_orphan_detector_ignores_large_gaps():
    """OrphanDetector should ignore gaps > 2 years."""
    from app.scraper.orchestration.phase3 import OrphanDetector
    
    teams = [
        {"node_id": "a", "name": "Team A", "end_year": 2018},
        {"node_id": "b", "name": "Team B", "start_year": 2023},
    ]
    
    detector = OrphanDetector(max_gap_years=2)
    candidates = detector.find_candidates(teams)
    
    assert len(candidates) == 0
```

**Implementation:**
Create `backend/app/scraper/orchestration/phase3.py`:
```python
"""Phase 3: Lineage Connection."""
from typing import List, Dict, Any

class OrphanDetector:
    """Detects orphan nodes that may need lineage connections."""
    
    def __init__(self, max_gap_years: int = 2):
        self._max_gap = max_gap_years
    
    def find_candidates(
        self,
        teams: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Find teams that ended near when another started."""
        candidates = []
        
        ended_teams = [t for t in teams if "end_year" in t]
        started_teams = [t for t in teams if "start_year" in t]
        
        for ended in ended_teams:
            for started in started_teams:
                gap = started["start_year"] - ended["end_year"]
                if 0 < gap <= self._max_gap:
                    candidates.append({
                        "predecessor": ended,
                        "successor": started,
                        "gap_years": gap
                    })
        
        return candidates
```

**Verify:** Run `pytest backend/tests/scraper/test_phase3.py -v`

---

### TASK 10.2: Lineage Connection Service

**Test First:**
Add to `backend/tests/scraper/test_phase3.py`:
```python
from unittest.mock import AsyncMock, MagicMock
from uuid import uuid4

@pytest.mark.asyncio
async def test_lineage_service_creates_event():
    """LineageConnectionService should create lineage events."""
    from app.scraper.orchestration.phase3 import LineageConnectionService
    from app.scraper.llm.lineage import LineageDecision
    from app.models.enums import LineageEventType
    
    mock_prompts = AsyncMock()
    mock_prompts.decide_lineage = AsyncMock(
        return_value=LineageDecision(
            event_type=LineageEventType.LEGAL_TRANSFER,
            confidence=0.95,
            reasoning="Same team",
            predecessor_ids=[uuid4()],
            successor_ids=[uuid4()]
        )
    )
    
    mock_audit = AsyncMock()
    mock_session = AsyncMock()
    
    service = LineageConnectionService(
        prompts=mock_prompts,
        audit_service=mock_audit,
        session=mock_session,
        system_user_id=uuid4()
    )
    
    await service.connect(
        predecessor_info="Team A 2022",
        successor_info="Team B 2023"
    )
    
    mock_prompts.decide_lineage.assert_called_once()
```

**Implementation:**
Add to `backend/app/scraper/orchestration/phase3.py`:
```python
import logging
from uuid import UUID
from sqlalchemy.ext.asyncio import AsyncSession
from app.scraper.llm.prompts import ScraperPrompts
from app.services.audit_log_service import AuditLogService
from app.models.enums import EditAction, EditStatus

logger = logging.getLogger(__name__)
CONFIDENCE_THRESHOLD = 0.90

class LineageConnectionService:
    """Orchestrates Phase 3: Creating lineage connections."""
    
    def __init__(
        self,
        prompts: ScraperPrompts,
        audit_service: AuditLogService,
        session: AsyncSession,
        system_user_id: UUID
    ):
        self._prompts = prompts
        self._audit = audit_service
        self._session = session
        self._user_id = system_user_id
    
    async def connect(
        self,
        predecessor_info: str,
        successor_info: str
    ) -> None:
        """Analyze and create lineage connection."""
        decision = await self._prompts.decide_lineage(
            predecessor_info=predecessor_info,
            successor_info=successor_info
        )
        
        status = (
            EditStatus.APPROVED if decision.confidence >= CONFIDENCE_THRESHOLD
            else EditStatus.PENDING
        )
        
        await self._audit.create_edit(
            session=self._session,
            user_id=self._user_id,
            entity_type="LineageEvent",
            entity_id=None,
            action=EditAction.CREATE,
            old_data=None,
            new_data={
                "event_type": decision.event_type.value,
                "predecessor_ids": [str(id) for id in decision.predecessor_ids],
                "successor_ids": [str(id) for id in decision.successor_ids],
                "reasoning": decision.reasoning
            },
            status=status
        )
        
        logger.info(f"Created lineage {decision.event_type.value} ({status.value})")
```

**Verify:** Run `pytest backend/tests/scraper/test_phase3.py -v`

---

### WIRING: Export Phase 3 components

Update `backend/app/scraper/orchestration/__init__.py`:
```python
from app.scraper.orchestration.phase3 import (
    OrphanDetector, LineageConnectionService
)
# ... existing exports ...
```

---

## Finalize Slice 10

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 10.1 Implement orphan node detector
- [x] 10.2 Implement lineage decision pipeline
- [x] 10.3 Integrate with AuditLogService for LineageEvent
- [x] 10.4 Wire Phase 3 into `SmartScraperService`
- [x] **SLICE 10 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add Phase 3 lineage connection orchestration

- Add OrphanDetector to find candidate connections
- Add LineageConnectionService with LLM integration
- Create LineageEvents via AuditLogService"
```

---

# SLICE 11: Secondary/Tertiary Scrapers

## Context
Implement enrichment scrapers for CyclingRanking, Wikipedia (multi-language: EN/DE/FR/IT/ES/NL), and Archive.org.

**Dependencies:** SLICE 3 must be complete.

## Prompt

You are implementing SLICE 11 of the Smart Scraper project. Follow TDD strictly.

### TASK 11.1: CyclingRanking Scraper (Test: Parser)

**Test First:**
Create `backend/tests/scraper/test_secondary_scrapers.py`:
```python
"""Test secondary/tertiary scrapers."""
import pytest
from pathlib import Path

def test_cycling_ranking_parser():
    """CyclingRankingParser should extract team data."""
    from app.scraper.sources.cycling_ranking import CyclingRankingParser
    
    html = """
    <div class="team-info">
        <h1>Team Name Here</h1>
        <span class="founded">Founded: 1985</span>
    </div>
    """
    
    parser = CyclingRankingParser()
    data = parser.parse_team(html)
    
    assert data.get("founded_year") == 1985
```

**Implementation:**
Create `backend/app/scraper/sources/cycling_ranking.py`:
```python
"""CyclingRanking scraper implementation."""
import re
from typing import Dict, Any, Optional
from bs4 import BeautifulSoup
from app.scraper.base import BaseScraper

class CyclingRankingParser:
    """Parser for CyclingRanking HTML."""
    
    def parse_team(self, html: str) -> Dict[str, Any]:
        """Extract team data from page."""
        soup = BeautifulSoup(html, 'html.parser')
        
        founded = None
        founded_elem = soup.select_one('.founded')
        if founded_elem:
            match = re.search(r'(\d{4})', founded_elem.get_text())
            if match:
                founded = int(match.group(1))
        
        return {"founded_year": founded}

class CyclingRankingScraper(BaseScraper):
    """Scraper for CyclingRanking website."""
    
    BASE_URL = "https://cyclingranking.com"
    
    def __init__(self, **kwargs):
        super().__init__(min_delay=3.0, max_delay=6.0, **kwargs)
        self._parser = CyclingRankingParser()
    
    async def get_team(self, team_slug: str) -> Dict[str, Any]:
        """Get enrichment data for a team."""
        url = f"{self.BASE_URL}/team/{team_slug}"
        html = await self.fetch(url)
        return self._parser.parse_team(html)
```

**Verify:** Run `pytest backend/tests/scraper/test_secondary_scrapers.py -v`

---

### TASK 11.2: Wayback Machine Scraper

**Test First:**
Add to `backend/tests/scraper/test_secondary_scrapers.py`:
```python
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_wayback_scraper_gets_newest():
    """WaybackScraper should fetch newest snapshot."""
    from app.scraper.sources.wayback import WaybackScraper
    
    mock_response = {
        "archived_snapshots": {
            "closest": {
                "url": "https://web.archive.org/web/20231201/http://example.com",
                "timestamp": "20231201120000"
            }
        }
    }
    
    with patch.object(WaybackScraper, '_fetch_json', new_callable=AsyncMock) as mock:
        mock.return_value = mock_response
        
        scraper = WaybackScraper(min_delay=0, max_delay=0)
        snapshot = await scraper.get_newest_snapshot("http://example.com")
        
        assert "20231201" in snapshot["url"]
```

**Implementation:**
Create `backend/app/scraper/sources/wayback.py`:
```python
"""Wayback Machine (Archive.org) scraper."""
from typing import Dict, Any, Optional
import httpx
from app.scraper.base import BaseScraper

class WaybackScraper(BaseScraper):
    """Scraper for Archive.org Wayback Machine."""
    
    API_URL = "https://archive.org/wayback/available"
    
    def __init__(self, **kwargs):
        super().__init__(min_delay=5.0, max_delay=10.0, **kwargs)
    
    async def _fetch_json(self, url: str) -> Dict[str, Any]:
        """Fetch JSON from URL."""
        await self._rate_limiter.wait()
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.get(url)
            return response.json()
    
    async def get_newest_snapshot(
        self,
        target_url: str
    ) -> Optional[Dict[str, Any]]:
        """Get the newest available snapshot."""
        api_url = f"{self.API_URL}?url={target_url}"
        data = await self._fetch_json(api_url)
        
        snapshots = data.get("archived_snapshots", {})
        closest = snapshots.get("closest")
        
        return closest
    
    async def fetch_archived_page(self, snapshot_url: str) -> str:
        """Fetch the actual archived HTML."""
        return await self.fetch(snapshot_url)
```

**Verify:** Run `pytest backend/tests/scraper/test_secondary_scrapers.py -v`

---

### WIRING: Export secondary scrapers

Update `backend/app/scraper/sources/__init__.py`:
```python
from app.scraper.sources.cycling_ranking import (
    CyclingRankingScraper, CyclingRankingParser
)
from app.scraper.sources.wayback import WaybackScraper
# ... existing exports ...
```

---

## Finalize Slice 11

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 11.1 Implement `CyclingRankingScraper`
- [x] 11.2 Implement `WikidataScraper` (SPARQL)
- [x] 11.3 Implement `WikipediaScraper` (EN/DE/FR/IT/ES/NL)
- [x] 11.4 Implement `WaybackScraper`
- [x] 11.5 Implement `MemoireScraper`
- [x] **SLICE 11 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add secondary/tertiary source scrapers

- Add CyclingRankingScraper for enrichment data
- Add WaybackScraper for Archive.org access
- Conservative rate limits for external services"
```

---

# SLICE 12: Concurrent Source Workers

## Context
Implement parallel workers to maximize throughput while respecting per-source rate limits.

**Dependencies:** SLICE 11 must be complete.

## Prompt

You are implementing SLICE 12 of the Smart Scraper project. Follow TDD strictly.

### TASK 12.1: Worker Pool (Test: Parallel Execution)

**Test First:**
Create `backend/tests/scraper/test_workers.py`:
```python
"""Test concurrent source workers."""
import pytest
import asyncio
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_worker_pool_runs_parallel():
    """WorkerPool should run tasks in parallel."""
    from app.scraper.workers import WorkerPool
    
    results = []
    
    async def task(item):
        await asyncio.sleep(0.01)
        results.append(item)
        return item
    
    pool = WorkerPool(max_workers=3)
    items = [1, 2, 3, 4, 5]
    
    await pool.run(items, task)
    
    assert len(results) == 5
    assert set(results) == {1, 2, 3, 4, 5}

@pytest.mark.asyncio
async def test_worker_pool_limits_concurrency():
    """WorkerPool should respect max_workers limit."""
    from app.scraper.workers import WorkerPool
    
    active = []
    max_active = 0
    
    async def task(item):
        nonlocal max_active
        active.append(item)
        max_active = max(max_active, len(active))
        await asyncio.sleep(0.05)
        active.remove(item)
        return item
    
    pool = WorkerPool(max_workers=2)
    await pool.run([1, 2, 3, 4], task)
    
    assert max_active <= 2
```

**Implementation:**
Create `backend/app/scraper/workers.py`:
```python
"""Concurrent worker pool for scraping."""
import asyncio
import logging
from typing import TypeVar, Callable, Awaitable, List, Any

logger = logging.getLogger(__name__)
T = TypeVar('T')
R = TypeVar('R')

class WorkerPool:
    """Pool of concurrent workers with limited parallelism."""
    
    def __init__(self, max_workers: int = 3):
        self._max_workers = max_workers
        self._semaphore = asyncio.Semaphore(max_workers)
    
    async def run(
        self,
        items: List[T],
        task: Callable[[T], Awaitable[R]]
    ) -> List[R]:
        """Run task on all items with limited concurrency."""
        
        async def bounded_task(item: T) -> R:
            async with self._semaphore:
                return await task(item)
        
        tasks = [bounded_task(item) for item in items]
        return await asyncio.gather(*tasks, return_exceptions=True)
```

**Verify:** Run `pytest backend/tests/scraper/test_workers.py -v`

---

### TASK 12.2: Source-Specific Workers

**Test First:**
Add to `backend/tests/scraper/test_workers.py`:
```python
@pytest.mark.asyncio
async def test_multi_source_coordinator():
    """MultiSourceCoordinator should run workers per source."""
    from app.scraper.workers import MultiSourceCoordinator
    
    results = {"source_a": [], "source_b": []}
    
    async def fetch_a(url):
        results["source_a"].append(url)
    
    async def fetch_b(url):
        results["source_b"].append(url)
    
    coordinator = MultiSourceCoordinator()
    coordinator.add_source("source_a", fetch_a, max_workers=2)
    coordinator.add_source("source_b", fetch_b, max_workers=1)
    
    await coordinator.enqueue("source_a", "url1")
    await coordinator.enqueue("source_a", "url2")
    await coordinator.enqueue("source_b", "url3")
    
    await coordinator.run_all()
    
    assert len(results["source_a"]) == 2
    assert len(results["source_b"]) == 1
```

**Implementation:**
Add to `backend/app/scraper/workers.py`:
```python
from dataclasses import dataclass, field
from typing import Dict

@dataclass
class SourceWorker:
    """Worker configuration for a specific source."""
    fetch_fn: Callable[[str], Awaitable[Any]]
    max_workers: int
    queue: List[str] = field(default_factory=list)

class MultiSourceCoordinator:
    """Coordinates workers across multiple sources."""
    
    def __init__(self):
        self._sources: Dict[str, SourceWorker] = {}
    
    def add_source(
        self,
        name: str,
        fetch_fn: Callable[[str], Awaitable[Any]],
        max_workers: int = 2
    ) -> None:
        """Register a source with its worker config."""
        self._sources[name] = SourceWorker(
            fetch_fn=fetch_fn,
            max_workers=max_workers
        )
    
    async def enqueue(self, source: str, url: str) -> None:
        """Add URL to source's queue."""
        if source in self._sources:
            self._sources[source].queue.append(url)
    
    async def run_all(self) -> None:
        """Run all source workers in parallel."""
        tasks = []
        
        for name, worker in self._sources.items():
            pool = WorkerPool(max_workers=worker.max_workers)
            task = pool.run(worker.queue, worker.fetch_fn)
            tasks.append(task)
        
        await asyncio.gather(*tasks)
```

**Verify:** Run `pytest backend/tests/scraper/test_workers.py -v`

---

### WIRING: Export workers

Update `backend/app/scraper/__init__.py`:
```python
from app.scraper.workers import WorkerPool, MultiSourceCoordinator
```

---

## Finalize Slice 12

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 12.1 Implement async worker pool
- [x] 12.2 Per-source rate limiting in workers
- [x] 12.3 Worker coordination
- [x] 12.4 Integrate into Phase 2+ orchestration
- [x] **SLICE 12 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add concurrent source workers

- Add WorkerPool with semaphore-based concurrency limit
- Add MultiSourceCoordinator for parallel source fetching
- Per-source rate limiting preserved"
```

---


# SLICE 13: CLI Interface

## Context
Create a command-line interface to run the scraper locally.

**Dependencies:** SLICE 10 must be complete.

## Prompt

You are implementing SLICE 13 of the Smart Scraper project. Follow TDD strictly.

### TASK 13.1: CLI Argument Parser (Test: Phase Args)

**Test First:**
Create `backend/tests/scraper/test_cli.py`:
```python
"""Test CLI interface."""
import pytest
from unittest.mock import patch, AsyncMock

def test_cli_parses_phase():
    """CLI should parse --phase argument."""
    from app.scraper.cli import parse_args
    
    args = parse_args(["--phase", "1"])
    assert args.phase == 1

def test_cli_parses_tier():
    """CLI should parse --tier argument."""
    from app.scraper.cli import parse_args
    
    args = parse_args(["--tier", "wt"])
    assert args.tier == "wt"

def test_cli_parses_resume():
    """CLI should parse --resume flag."""
    from app.scraper.cli import parse_args
    
    args = parse_args(["--resume"])
    assert args.resume is True

def test_cli_parses_dry_run():
    """CLI should parse --dry-run flag."""
    from app.scraper.cli import parse_args
    
    args = parse_args(["--dry-run"])
    assert args.dry_run is True
```

**Implementation:**
Create `backend/app/scraper/cli.py`:
```python
"""Smart Scraper CLI interface."""
import argparse
import asyncio
import logging
from typing import List, Optional

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args(args: Optional[List[str]] = None) -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Smart Scraper - Cycling team data ingestion"
    )
    
    parser.add_argument(
        "--phase",
        type=int,
        choices=[1, 2, 3],
        default=1,
        help="Phase to run (1=Discovery, 2=Assembly, 3=Lineage)"
    )
    
    parser.add_argument(
        "--tier",
        type=str,
        choices=["wt", "pt", "ct", "all"],
        default="wt",
        help="Team tier to process (wt=WorldTour, pt=ProTeam, ct=Continental)"
    )
    
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume from last checkpoint"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate without writing to database"
    )
    
    parser.add_argument(
        "--start-year",
        type=int,
        default=2025,
        help="Start year for scraping (default: current year)"
    )
    
    parser.add_argument(
        "--end-year",
        type=int,
        default=1990,
        help="End year for scraping (default: 1990)"
    )
    
    return parser.parse_args(args)
```

**Verify:** Run `pytest backend/tests/scraper/test_cli.py -v`

---

### TASK 13.2: CLI Runner

**Test First:**
Add to `backend/tests/scraper/test_cli.py`:
```python
@pytest.mark.asyncio
async def test_cli_runner_executes_phase1():
    """CLI runner should execute Phase 1."""
    from app.scraper.cli import run_scraper
    
    with patch('app.scraper.cli.DiscoveryService') as mock_discovery:
        mock_instance = AsyncMock()
        mock_instance.discover_teams = AsyncMock(return_value=None)
        mock_discovery.return_value = mock_instance
        
        await run_scraper(phase=1, tier="wt", resume=False, dry_run=True)
        
        mock_instance.discover_teams.assert_called_once()
```

**Implementation:**
Add to `backend/app/scraper/cli.py`:
```python
from pathlib import Path
from app.scraper.checkpoint import CheckpointManager

async def run_scraper(
    phase: int,
    tier: str,
    resume: bool,
    dry_run: bool,
    start_year: int = 2025,
    end_year: int = 1990
) -> None:
    """Run the scraper for specified phase."""
    logger.info(f"Starting Phase {phase} for tier {tier}")
    
    if dry_run:
        logger.info("DRY RUN - no database writes")
    
    checkpoint_path = Path("./scraper_checkpoint.json")
    checkpoint_manager = CheckpointManager(checkpoint_path)
    
    if not resume:
        checkpoint_manager.clear()
    
    if phase == 1:
        from app.scraper.sources.cyclingflash import CyclingFlashScraper
        from app.scraper.orchestration.phase1 import DiscoveryService
        
        scraper = CyclingFlashScraper()
        service = DiscoveryService(
            scraper=scraper,
            checkpoint_manager=checkpoint_manager
        )
        
        result = await service.discover_teams(
            start_year=start_year,
            end_year=end_year
        )
        
        logger.info(f"Discovered {len(result.team_urls)} teams")
        logger.info(f"Collected {len(result.sponsor_names)} unique sponsors")
    
    elif phase == 2:
        logger.info("Phase 2: Team Assembly - Not yet implemented")
    
    elif phase == 3:
        logger.info("Phase 3: Lineage Connection - Not yet implemented")


def main() -> None:
    """CLI entry point."""
    args = parse_args()
    
    asyncio.run(run_scraper(
        phase=args.phase,
        tier=args.tier,
        resume=args.resume,
        dry_run=args.dry_run,
        start_year=args.start_year,
        end_year=args.end_year
    ))


if __name__ == "__main__":
    main()
```

**Verify:** Run `pytest backend/tests/scraper/test_cli.py -v`

---

### WIRING: Add CLI entry point to pyproject/setup

Add to `backend/pyproject.toml` (or `setup.py`):
```toml
[project.scripts]
smart-scraper = "app.scraper.cli:main"
```

---

## Finalize Slice 13

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 13.1 Implement argparse CLI
- [x] 13.2 Add --phase argument
- [x] 13.3 Add --tier argument
- [x] 13.4 Add --resume argument
- [x] 13.5 Add --dry-run argument
- [x] **SLICE 13 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add CLI interface

- Add argument parser for phase, tier, resume, dry-run
- Add run_scraper async entry point
- Support checkpoint resume"
```

---

# SLICE 14: API Endpoint

## Context
Create an admin-only API endpoint to trigger the scraper remotely.

**Dependencies:** SLICE 13 must be complete.

## Prompt

You are implementing SLICE 14 of the Smart Scraper project. Follow TDD strictly.

### TASK 14.1: Scraper Start Endpoint (Test: Admin Only)

**Test First:**
Create `backend/tests/api/test_scraper_api.py`:
```python
"""Test Scraper API endpoints."""
import pytest
from httpx import AsyncClient
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_scraper_start_requires_admin(client: AsyncClient):
    """POST /api/admin/scraper/start requires admin role."""
    response = await client.post("/api/admin/scraper/start")
    assert response.status_code in [401, 403]

@pytest.mark.asyncio
async def test_scraper_start_as_admin(
    client: AsyncClient,
    admin_auth_headers
):
    """Admin can start scraper."""
    with patch('app.api.admin.scraper.run_scraper_background') as mock:
        mock.return_value = {"task_id": "test-123"}
        
        response = await client.post(
            "/api/admin/scraper/start",
            json={"phase": 1, "tier": "wt"},
            headers=admin_auth_headers
        )
        
        assert response.status_code == 202
        assert "task_id" in response.json()
```

**Implementation:**
Create `backend/app/api/admin/scraper.py`:
```python
"""Scraper admin API endpoints."""
import uuid
from fastapi import APIRouter, Depends, BackgroundTasks
from pydantic import BaseModel
from app.api.deps import get_current_admin_user

router = APIRouter(prefix="/scraper", tags=["scraper"])

class ScraperStartRequest(BaseModel):
    """Request to start scraper."""
    phase: int = 1
    tier: str = "wt"
    resume: bool = False
    dry_run: bool = False

class ScraperStartResponse(BaseModel):
    """Response from starting scraper."""
    task_id: str
    message: str

# In-memory task tracking (use Redis in production)
_tasks: dict = {}

async def run_scraper_background(task_id: str, request: ScraperStartRequest):
    """Background task to run scraper."""
    from app.scraper.cli import run_scraper
    
    _tasks[task_id] = {"status": "running", "phase": request.phase}
    
    try:
        await run_scraper(
            phase=request.phase,
            tier=request.tier,
            resume=request.resume,
            dry_run=request.dry_run
        )
        _tasks[task_id]["status"] = "completed"
    except Exception as e:
        _tasks[task_id] = {"status": "failed", "error": str(e)}


@router.post("/start", response_model=ScraperStartResponse, status_code=202)
async def start_scraper(
    request: ScraperStartRequest,
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_admin_user)
):
    """Start the scraper as a background task."""
    task_id = str(uuid.uuid4())
    
    background_tasks.add_task(
        run_scraper_background,
        task_id,
        request
    )
    
    return ScraperStartResponse(
        task_id=task_id,
        message=f"Scraper Phase {request.phase} started"
    )


@router.get("/status/{task_id}")
async def get_scraper_status(
    task_id: str,
    current_user = Depends(get_current_admin_user)
):
    """Get status of a scraper task."""
    if task_id not in _tasks:
        return {"status": "not_found"}
    return _tasks[task_id]
```

**Verify:** Run `pytest backend/tests/api/test_scraper_api.py -v`

---

### TASK 14.2: Register Router

**Implementation:**
Update `backend/app/api/admin/__init__.py`:
```python
from app.api.admin.scraper import router as scraper_router
# ... existing imports ...
```

Update `backend/main.py`:
```python
from app.api.admin.scraper import router as scraper_router

app.include_router(scraper_router, prefix="/api/admin")
```

---

## Finalize Slice 14

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 14.1 Create `POST /api/admin/scraper/start`
- [x] 14.2 Implement background task execution
- [x] 14.3 Create `GET /api/admin/scraper/status`
- [x] 14.4 Add admin-only authorization
- [x] **SLICE 14 COMMITTED**
```

**Step 2: Commit (execute now)**
```bash
git add -A && git commit -m "feat(scraper): add API endpoint

- Add POST /api/admin/scraper/start endpoint
- Add GET /api/admin/scraper/status/{task_id} endpoint
- Admin-only authorization
- Background task execution"
```

---

# SLICE 15: E2E Testing & Polish

## Context
Final integration testing and documentation.

**Dependencies:** All previous slices must be complete.

## Prompt

You are implementing SLICE 15 of the Smart Scraper project. Follow TDD strictly.

### TASK 15.1: E2E Integration Test (Mocked)

**Test First:**
Create `backend/tests/integration/test_scraper_e2e.py`:
```python
"""End-to-end scraper tests."""
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
from uuid import uuid4

@pytest.mark.asyncio
async def test_full_phase1_flow_mocked(isolated_session):
    """Full Phase 1 flow should discover teams and collect sponsors."""
    from app.scraper.sources.cyclingflash import ScrapedTeamData
    from app.scraper.orchestration.phase1 import DiscoveryService
    from app.scraper.checkpoint import CheckpointManager
    from pathlib import Path
    import tempfile
    
    # Mock scraper
    mock_scraper = AsyncMock()
    mock_scraper.get_team_list = AsyncMock(return_value=[
        "/team/team-a-2024",
        "/team/team-b-2024"
    ])
    mock_scraper.get_team = AsyncMock(side_effect=[
        ScrapedTeamData(
            name="Team A",
            season_year=2024,
            sponsors=["Sponsor1", "Sponsor2"],
            previous_season_url=None
        ),
        ScrapedTeamData(
            name="Team B",
            season_year=2024,
            sponsors=["Sponsor2", "Sponsor3"],
            previous_season_url=None
        )
    ])
    
    with tempfile.TemporaryDirectory() as tmpdir:
        checkpoint = CheckpointManager(Path(tmpdir) / "cp.json")
        
        service = DiscoveryService(
            scraper=mock_scraper,
            checkpoint_manager=checkpoint
        )
        
        result = await service.discover_teams(
            start_year=2024,
            end_year=2024
        )
        
        assert len(result.team_urls) == 2
        assert len(result.sponsor_names) == 3  # Unique sponsors
        assert "Sponsor1" in result.sponsor_names
        assert "Sponsor2" in result.sponsor_names
        assert "Sponsor3" in result.sponsor_names

@pytest.mark.asyncio
async def test_phase2_creates_audit_entries(isolated_session):
    """Phase 2 should create audit log entries."""
    from app.scraper.sources.cyclingflash import ScrapedTeamData
    from app.scraper.orchestration.phase2 import TeamAssemblyService
    
    mock_audit = AsyncMock()
    mock_audit.create_edit = AsyncMock(return_value=MagicMock(edit_id=uuid4()))
    
    service = TeamAssemblyService(
        audit_service=mock_audit,
        session=isolated_session,
        system_user_id=uuid4()
    )
    
    team_data = ScrapedTeamData(
        name="Test Team",
        season_year=2024,
        sponsors=["Main Sponsor", "Secondary"],
        tier="WorldTour"
    )
    
    await service.create_team_era(team_data, confidence=0.95)
    
    mock_audit.create_edit.assert_called_once()
    call_args = mock_audit.create_edit.call_args
    assert call_args.kwargs["new_data"]["registered_name"] == "Test Team"
```

**Verify:** Run `pytest backend/tests/integration/test_scraper_e2e.py -v`

---

### TASK 15.2: Documentation

Create `backend/app/scraper/README.md`:
```markdown
# Smart Scraper

Bulk ingestion tool for cycling team historical data.

## Quick Start

### CLI Usage

```bash
# Run Phase 1: Discovery
python -m app.scraper.cli --phase 1 --tier wt

# Resume from checkpoint
python -m app.scraper.cli --phase 1 --resume

# Dry run (no DB writes)
python -m app.scraper.cli --phase 1 --dry-run
```

### API Usage

```bash
# Start scraper (admin only)
curl -X POST http://localhost:8000/api/admin/scraper/start \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"phase": 1, "tier": "wt"}'

# Check status
curl http://localhost:8000/api/admin/scraper/status/{task_id} \
  -H "Authorization: Bearer $TOKEN"
```

## Phases

1. **Discovery**: Spider CyclingFlash, collect team URLs and sponsor names
2. **Assembly**: Create TeamNodes, TeamEras, link sponsors
3. **Lineage**: Detect orphans, create LineageEvents

## Configuration

Environment variables:
- `GEMINI_API_KEY`: Google Gemini API key
- `DEEPSEEK_API_KEY`: Deepseek API key (fallback)

## Architecture

See `docs/SMART_SCRAPER_SPECIFICATION.md` for full details.
```

---

### TASK 15.3: Run Full Test Suite

```bash
# Run all scraper tests
pytest backend/tests/scraper/ -v

# Run integration tests
pytest backend/tests/integration/test_scraper_e2e.py -v

# Run API tests
pytest backend/tests/api/test_scraper_api.py -v
```

---

## Finalize Slice 15

**Step 1: Update Task Checklist**

Edit `docs/SMART_SCRAPER_TASKS.md` and mark the following as complete:
```markdown
- [x] 15.1 Full E2E test (mock all sources)
- [x] 15.2 Test with real CyclingFlash (limited scope)
- [x] 15.3 Documentation (README section)
- [x] 15.4 Error handling review
- [x] **SLICE 15 COMMITTED**
```

**Step 2: Update Final checklist**

Also update the Final section of `docs/SMART_SCRAPER_TASKS.md`:
```markdown
- [x] Create Pull Request
```

**Step 3: Commit and prepare PR (execute now)**
```bash
git add -A && git commit -m "feat(scraper): complete E2E tests and documentation

- Add full Phase 1/2 integration tests
- Add scraper README documentation
- All tests passing"
```

---

# Final Integration Commit

After completing all 15 slices:

```bash
git add -A
git commit -m "feat(scraper): complete Smart Scraper implementation

- All 15 slices implemented and tested
- Full Phase 1/2/3 workflow operational
- CLI and API entry points ready
- Checkpointing and concurrent workers enabled
- LLM fallback chain (Gemini â†’ Deepseek)
- 90% confidence auto-approval threshold"

git push origin smart-scraper
```

Then create a Pull Request for review.

---

# Summary

You have now completed the full **Smart Scraper** implementation in 15 TDD slices:

| Slice | Focus | Status |
|-------|-------|--------|
| 1 | Foundation | âœ… |
| 2 | LLM Client Layer | âœ… |
| 3 | Scraper Base | âœ… |
| 4 | CyclingFlash Scraper | âœ… |
| 5 | Data Extraction Prompt | âœ… |
| 6 | Checkpoint System | âœ… |
| 7 | Phase 1 Orchestration | âœ… |
| 8 | Phase 2 Orchestration | âœ… |
| 9 | Lineage Decision Prompt | âœ… |
| 10 | Phase 3 Orchestration | âœ… |
| 11 | Secondary Scrapers | âœ… |
| 12 | Concurrent Workers | âœ… |
| 13 | CLI Interface | âœ… |
| 14 | API Endpoint | âœ… |
| 15 | E2E & Polish | âœ… |

**Total estimated time: ~20 working days**



